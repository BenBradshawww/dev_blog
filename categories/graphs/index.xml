<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Graphs on My New Hugo Site</title>
    <link>https://example.org/categories/graphs/</link>
    <description>Recent content in Graphs on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/categories/graphs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Message Passing Neural Networks</title>
      <link>https://example.org/posts/message-passing-neural-networks/</link>
      <pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/message-passing-neural-networks/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/boliviainteligente-AXNZb4FEkh8-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@boliviainteligente&#34;&gt;Image by BoliviaInteligente&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Over the past 15 years we have seen a surge of use of Graph Neural Networks (GNNs) being used to model social networks, recommendations systems, transportation networks, and many more systems. With the ever growing use of GNNs, this has naturally led to the questions about the use of GNNs within the medical sector. More specifically, the question of using GNNs to predict the properties of molecules was brought up. Back in the early 2010s, this idea was in its infancy with few successful applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graph Sage</title>
      <link>https://example.org/posts/graphsage/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/graphsage/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/cajeo-zhang-20JfNRPsMCo-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@cajeo?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Cajeo Zhang&lt;/a&gt;&#xA;Graphs have been used across many fields due to their ability to represent relationships between entities with applications including social networks, search engines, and protein-protein interaction networks. However, one growing limitation of these graphs are the amount of computational resources they require with some large-scale graphs having millions of nodes each with their own set of features and their set of edges.&lt;/p&gt;&#xA;&lt;p&gt;This has led to the creation of graph embedding methods, more specifically the deep embedding methods. These embedding methods aim to create a high-quality representation of the nodes and their edges. Rather than just incorporating the graph structural information into an embedding, these methods also include node and edges features and other hierarchical information. This results in a complicated model which are able to learn very rich representations of nodes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graph Factorisation Methods in Shallow Graphs</title>
      <link>https://example.org/posts/graph-factorisation-methods-in-shallow-graphs/</link>
      <pubDate>Sat, 20 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/graph-factorisation-methods-in-shallow-graphs/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/photo-1706391162070-60a37dee114d.avif&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://unsplash.com/@miracleday?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Elena Mozhvilo&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;&#xA;&lt;p&gt;Graphs are incredibly useful for modelling a range of relationships and interactions. Using nodes to represent entities and edges to represent connections between these entities, they have become a very useful representation tool. Nowadays they are used to model social networks, protein-protein interactions, recommendations systems, knowledge graphs, supply chains, and so much more. However, as these graphs scale up and add more nodes and edges, a range of issues start to arise. They start to become computationally expensive to process, noisy, and difficult to interpret.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Instance Normalisation within GANs</title>
      <link>https://example.org/posts/instance-normalisation-within-gans/</link>
      <pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/instance-normalisation-within-gans/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/justin-simmonds-oGKncrpXn70-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@justsimms?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Justin Simmonds&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Generative Adversarial Networks (GANs) were first introduced in 2014 by Ian Goodfellow in his paper “Generative Adversarial Nets.” This paper presented the GAN framework, which consists of two neural networks called the generator and the discriminator. The generator takes random noise as input and outputs a generated image. The discriminator takes both a generated image and a real image as inputs and tries to determine which is real and which is generated. Their training process can be likened to a ping-pong game, with the generator trying to produce images that fool the discriminator, and the discriminator is trying to identify which images are generated.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
