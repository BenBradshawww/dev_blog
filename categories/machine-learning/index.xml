<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on My New Hugo Site</title>
    <link>https://example.org/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>XGBoost</title>
      <link>https://example.org/posts/xgboost/</link>
      <pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/xgboost/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;XGBoost&amp;rsquo;&#xA;date = 2024-12-26&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;tags = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Boosting&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;Decision Trees&amp;rdquo;, &amp;ldquo;Boosting&amp;rdquo;, &amp;ldquo;Gradient Boosting&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;XGBoost&amp;rsquo;&#xA;date = 2024-12-26&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;tags = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Boosting&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;Decision Trees&amp;rdquo;, &amp;ldquo;Boosting&amp;rdquo;, &amp;ldquo;Gradient Boosting&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kullback-Leibler Divergence</title>
      <link>https://example.org/posts/kullback-leibler-divergence/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/kullback-leibler-divergence/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/daniela-turcanu-wABdckxoj90-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@protopopica?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Daniela Turcanu&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article will cover the key features of Kullback-Leibler Divergence (KL divergence), a formula invented in 1951 by the mathematicians Soloman Kullback and Richard Leibler. This formula is used in the background of many of the modern day machine learning models focused around probabilistic modelling. These including Variational Autoencoders (VAEs), Generative Models, Reinforcement Learning, and Natural Language Processing. Additionally, this article will cover some of KL divergence&amp;rsquo;s key properties and briefly cover one of its applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>U-Net</title>
      <link>https://example.org/posts/u-net/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/u-net/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;U-Net&amp;rsquo;&#xA;date = 2024-07-03&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Auto-Encoders&amp;rdquo;]&#xA;tags = [&amp;ldquo;Auto-Encoders&amp;rdquo;]&#xA;keywords = [&amp;ldquo;U-Net&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;AutoEncoders&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;U-Net&amp;rsquo;&#xA;date = 2024-07-03&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Auto-Encoders&amp;rdquo;]&#xA;tags = [&amp;ldquo;Auto-Encoders&amp;rdquo;]&#xA;keywords = [&amp;ldquo;U-Net&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;AutoEncoders&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why Do Trees Outperform Neural Networks on Tabular Data?</title>
      <link>https://example.org/posts/why-do-trees-outperform-neural-networks-on-tabular-data/</link>
      <pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/why-do-trees-outperform-neural-networks-on-tabular-data/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Why Do Trees Outperform Neural Networks on Tabular Data?&amp;rsquo;&#xA;date = 2024-06-27&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;tags = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;Random Forests&amp;rdquo;, &amp;ldquo;Random Forest&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Why Do Trees Outperform Neural Networks on Tabular Data?&amp;rsquo;&#xA;date = 2024-06-27&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;tags = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;Random Forests&amp;rdquo;, &amp;ldquo;Random Forest&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non-Convex Optimisation Learning Rate Scheduling</title>
      <link>https://example.org/posts/non-convex-optimisation-learning-rate-scheduling/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/non-convex-optimisation-learning-rate-scheduling/</guid>
      <description>&lt;p&gt;+++&#xA;title = &amp;lsquo;Non-Convex Optimisation Learning Rate Scheduling&amp;rsquo;&#xA;draft = false&#xA;date = &amp;lsquo;2024-06-20&amp;rsquo;&#xA;image = &amp;ldquo;/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&amp;rdquo;&#xA;categories = [&amp;ldquo;Programming&amp;rdquo;, &amp;ldquo;AI&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;]&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Non-Convex Optimisation Learning Rate Scheduling&amp;rsquo;&#xA;draft = false&#xA;date = &amp;lsquo;2024-06-20&amp;rsquo;&#xA;image = &amp;ldquo;/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&amp;rdquo;&#xA;categories = [&amp;ldquo;Programming&amp;rdquo;, &amp;ldquo;AI&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;]&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://example.org/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@joshuas?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Joshua Sukoff&lt;/a&gt;&#xA;One of the most crucial hyperparameters in any machine learning (ML) model is the learning rate. A small learning rate often results in longer training times and can lead to overfitting. Conversely, a large learning rate may accelerate initial training but risks hindering the modelâ€™s convergence to the global minimum and can even cause divergence. Therefore, selecting the appropriate learning rate is a critical step in training any ML model.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
