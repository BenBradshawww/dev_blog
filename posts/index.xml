<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on My New Hugo Site</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gambler&#39;s Ruin for a Symmetric Random Walk</title>
      <link>http://localhost:1313/posts/gamblers-ruin-for-a-symmetric-random-walk/</link>
      <pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gamblers-ruin-for-a-symmetric-random-walk/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/jakob-owens-EroR0Ah9hqI-unsplash.jpg&#34; alt=&#34;Image by Jacob Owens: https://unsplash.com/@jakobowens1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;A gambler starts with i dollars and bets $1 on a fair coin toss, winning $1 on heads and losing $1 on tails. The game stops when the gambler reaches 0 or N dollars.&lt;/p&gt;&#xA;&lt;p&gt;• Derive the probability $p_i$ that the gambler reaches N dollars (i.e. “wins”) before going broke.&lt;/p&gt;&#xA;&lt;p&gt;We&amp;rsquo;ll cover 3 ways to approach this questions.&lt;/p&gt;&#xA;&lt;h2 id=&#34;solution-1&#34;&gt;Solution 1&lt;/h2&gt;&#xA;&lt;p&gt;Let $p_i$ denote the probability of reaching N dollars given he currently has i dollars.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some Questions in Probability II</title>
      <link>http://localhost:1313/posts/some-questions-in-probability-ii/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/some-questions-in-probability-ii/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/hannes-knutsson-2icXZAriT_E-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://unsplash.com/@hannesknutsson&#34;&gt;Image by Hannes Knutsson&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;question&#34;&gt;Question:&lt;/h3&gt;&#xA;&lt;p&gt;On average, how many rolls would it take to get two of the same face when rolling a die.&lt;/p&gt;&#xA;&lt;p&gt;For example, acceptable sequences with two of the same face include 2344, 1234566, and 123453. [1]&lt;/p&gt;&#xA;&lt;h3 id=&#34;solution&#34;&gt;Solution:&lt;/h3&gt;&#xA;&lt;p&gt;There are two ways you could approach this question, either using markov chains or permutations. Of these, we will focus on the latter, the easier of the two.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Terminating Dice Sequences</title>
      <link>http://localhost:1313/posts/terminating-dice-sequences/</link>
      <pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/terminating-dice-sequences/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/robin-noguier-sydwCr54rf0-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@robinnoguier&#34;&gt;Image by Robin Nouguier&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;question&#34;&gt;Question&lt;/h3&gt;&#xA;&lt;p&gt;What is the expected number of dice that need to be rolled till the sum exceeds 6? [1]&lt;/p&gt;&#xA;&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;&#xA;&lt;p&gt;To do this, we&amp;rsquo;ll start with the formula for expectation for discrete random variables:&lt;/p&gt;&#xA;$$\mathbb{E}[X] = \sum_{i=1}^6 \mathbb{P}[X = i]$$&lt;p&gt;There is a neat trick you can use where:&lt;/p&gt;&#xA;$$\begin{align}\mathbb{E}[X] &amp;= 1 \cdot \mathbb{P}[X=1] + 2 \cdot \mathbb{P}[X=2] + 3 \cdot \mathbb{P}[X=3] + 4 \cdot \mathbb{P}[X=4]  + 5 \cdot \mathbb{P}[X=5] + 6 \cdot \mathbb{P}[X=6] \\ &amp;=\mathbb{P}[X=1] \\ &amp;+ \mathbb{P}[X=2] + \mathbb{P}[X=2] \\ &amp;+ \mathbb{P}[X=3] + \mathbb{P}[X=3] + \mathbb{P}[X=3] \\ &amp;+ \mathbb{P}[X=4] + \mathbb{P}[X=4] + \mathbb{P}[X=4] + \mathbb{P}[X=4] \\ &amp;+ \mathbb{P}[X=5] + \mathbb{P}[X=5] + \mathbb{P}[X=5] + \mathbb{P}[X=5]+ \mathbb{P}[X=5] \\ &amp;+ \mathbb{P}[X=6]+\mathbb{P}[X=6]+\mathbb{P}[X=6]+\mathbb{P}[X=6]+\mathbb{P}[X=6]+\mathbb{P}[X=6]. \\ &amp;= \mathbb{P}[X&gt;0]+\mathbb{P}[X&gt;1]+\mathbb{P}[X&gt;2]+\mathbb{P}[X&gt;3]+\mathbb{P}[X&gt;4]+\mathbb{P}[X&gt;5]\end{align}$$&lt;p&gt;&#xA;This gives a new formula for expectation written as:&lt;/p&gt;</description>
    </item>
    <item>
      <title>XGBoost</title>
      <link>http://localhost:1313/posts/xgboost/</link>
      <pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/xgboost/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/bob-brewer-f309SjsrC3I-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@brewbottle&#34;&gt;Image by Bob Brewer&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Since the introduction of gradient tree boosting methods, tree boosting methods have often been the highest performing models across many tabular benchmarks. Alongside this, they have often be the winning models in many Kaggle competitions. Today we&amp;rsquo;ll cover the most popular of these models: XGBoost.&lt;/p&gt;&#xA;&lt;p&gt;XGBoost popularity stems from many reasons, with the most important being its scalability to all scenarios. It is one of the fastest tree based models to train because of the algorithms used with sparse data and it&amp;rsquo;s exploitation of parallel and distributed computing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Perpetual Children</title>
      <link>http://localhost:1313/posts/perpetual-children/</link>
      <pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/perpetual-children/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/vivek-kumar-riAKT8Z7ifE-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@vikceo&#34;&gt;Image by Vivek Kumar&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;The Question&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;A couple chooses to keep having children till they have an equal number of boys and girls. How many children will they have on average? [1]&lt;/p&gt;&#xA;&lt;p&gt;In this article we will cover 2 ways to solve this problem. First using a random walk and proof by contradiction. The second is Optiver&amp;rsquo;s solution to this problem which use Dyck&amp;rsquo;s path.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Solution 1&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Let $X_t$ denote the random variable for which child has been born at time $t$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some Questions in Probability I</title>
      <link>http://localhost:1313/posts/some-questions-in-probability-i/</link>
      <pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/some-questions-in-probability-i/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/cameron-cress-OZZIrjLTmA8-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@cam_cress&#34;&gt;Image by Cameron Cress&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-drunkard&#34;&gt;The Drunkard&lt;/h2&gt;&#xA;&lt;p&gt;Suppose someone has had a heavy evening in the pub and upon leaving the pub they have somehow ended up by the side of a cliff. Since he has one a one too many drinks he can&amp;rsquo;t walk well and they now has 1/3 probability to walk off the edge of the cliff or 2/3 probability to take a step backwards. What is the probability that he will eventually fall off the edge of the cliff?&lt;/p&gt;</description>
    </item>
    <item>
      <title>GAMs</title>
      <link>http://localhost:1313/posts/gams/</link>
      <pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gams/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/anh-vy--kJ542I1oVU-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@anhvygor&#34;&gt;Image by Anh Vy.&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this article we will cover Generalised Additive Models (GAMs). We&amp;rsquo;ll cover the base case by modelling a response variable with a univariate smooth function. We&amp;rsquo;ll then build on this by incorporating multiple exogenous variables to create additive models. After then, the GAM can be covered.&lt;/p&gt;&#xA;&lt;p&gt;For more details about this methods please read the book by Simon N. Wood about Generalised Additive Models.&lt;/p&gt;&#xA;&lt;h2 id=&#34;gams&#34;&gt;GAMs&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;GAMS:&lt;/strong&gt;&lt;/strong&gt; GAMs are a form of generalised linear models with linear response variables that depend on unknown smooth functions of some exogenous variables. These forms of models were initially developed to blend the properties of Generalised Linear Models (GLMs) with additive models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Docker</title>
      <link>http://localhost:1313/posts/docker/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/docker/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/will-turner-iSulLiXzGg0-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@turner_imagery?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Will Turner&lt;/a&gt;&#xA;In this article, we will cover what containerisation means and we look at Docker, a containerisation platform. Furthermore, we will cover the key commands and concepts needed to create your own containers in docker.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-docker&#34;&gt;What is Docker?&lt;/h2&gt;&#xA;&lt;p&gt;Docker is an open-source software platform that assists the deployment of applications. It does this by creating standardised units called containers which are isolated environments containing the application code, runtime, libraries, and any dependencies. These containers are a type of virtual machine that has an OS but does not simulate the entire computer. It is like a sandboxed environment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Martingale Theory</title>
      <link>http://localhost:1313/posts/martingale-theory/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/martingale-theory/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/sq-lim-aWm7fAUNPEc-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@sql&#34;&gt;Image by sq lim&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Today&amp;rsquo;s article will look at one of the more difficult areas in probability called martingale theory. We will cover the basic theory and look at 2 examples which use martingale theory (the drunk man and the ABRACADABRA problem).&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-theory&#34;&gt;The Theory&lt;/h2&gt;&#xA;&lt;p&gt;Before jumping into the problem, we have to cover some of the theory behind martingales. It is important to mention we will be skipping over many of the measure theory definitions and theorems. These will be covered at a later time. If you wish to read up more about this area of mathematics, I&amp;rsquo;d recommend reading the paper by T. Smith &lt;a href=&#34;https://math.uchicago.edu/~may/REU2012/REUPapers/Smith.pdf&#34;&gt;here&lt;/a&gt;. This paper was the basis for this article [1].&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Ant Problem</title>
      <link>http://localhost:1313/posts/the-ant-problem/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/the-ant-problem/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/mi_shots-8SJwz4nk7FA-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@mi_shots?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by mi_shots&lt;/a&gt;&#xA;Today we&amp;rsquo;ll be looking at a classic problem in probability: &amp;ldquo;The Ant Problem&amp;rdquo;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-question&#34;&gt;The Question:&lt;/h2&gt;&#xA;&lt;p&gt;An ant leaves its anthill in order to forage for food. It moves with the speed of 10cm per second, but it doesn&amp;rsquo;t know where to go, therefore every second it moves randomly 10cm directly north, south, east or west with equal probability.&lt;/p&gt;&#xA;&lt;p&gt;a) If the food is located on east-west lines 20cm to the north and 20cm to the south, as well as on north-south lines 20cm to the east and 20cm to the west from the anthill, how long will it take the ant to reach it on average?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rest APIs</title>
      <link>http://localhost:1313/posts/rest-apis/</link>
      <pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/rest-apis/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/ante-hamersmit-DSUjNkiRFg0-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@ante_kante?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Ante Hamersmit&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-are-apis&#34;&gt;What are APIs&lt;/h2&gt;&#xA;&lt;p&gt;Before we just into the class of Rest APIs it is best to understand what is an API. An API is simply an acronym for application programming interface, a software intermediary that allows two applications to talk to each other.&lt;/p&gt;&#xA;&lt;p&gt;They are usually explained in terms of a client and a server. The application that sends a request is called the client and the application that sends the response is called the server.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ARMA &amp; ARIMA</title>
      <link>http://localhost:1313/posts/arma--arima/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/arma--arima/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/elijah-grimm-jSt_ZxlG2bk-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@watchelijah?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Elijah Grimm&lt;/a&gt;&#xA;In this article we will cover the ARMA and ARIMA based statistical models, cover the maths behind these models, and look at the statistical test required prior and the post the model fitting.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;d recommend reading the previous article &lt;a href=&#34;https://mathstoml.ghost.io/what-is-a-time-series-how-can-we-model-it-2/&#34;&gt;here&lt;/a&gt; before reading the remainder of this article. In that article we cover the basics of what is a time series and how we could model it.&lt;/p&gt;&#xA;&lt;h3 id=&#34;definitions&#34;&gt;Definitions:&lt;/h3&gt;&#xA;&lt;p&gt;Recall from our previous article that given a sequence of data points $\{ X_{t}, t \in \mathbb{Z} \}$, the sequence is &lt;strong&gt;&lt;strong&gt;strongly stationary&lt;/strong&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;strong&gt;strictly stationary&lt;/strong&gt;&lt;/strong&gt; if&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is a Time Series &amp; How can we Model it?</title>
      <link>http://localhost:1313/posts/what-is-a-time-series--how-can-we-model-it/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/what-is-a-time-series--how-can-we-model-it/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/oguzhan-kiran-wj0l2BJKmkU-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://mathstoml.ghost.io/ghost/#/editor/post/66eb4a6473445600016630bb&#34;&gt;Image by Oğuzhan Kıran&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article will cover what is a time series, what does it mean for a time series to be stationary, and how could an autoregressive process or moving average process be used to model the time series.&lt;/p&gt;&#xA;&lt;p&gt;Before starting this article, I would like to mention the Cambridge notes from &lt;a href=&#34;https://www.statslab.cam.ac.uk/~rrw1/timeseries/t.pdf&#34;&gt;here&lt;/a&gt; form the foundation of this content.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-a-time-series&#34;&gt;What is a Time Series?&lt;/h2&gt;&#xA;&lt;p&gt;In short, time series is area of statistics which focuses on describing a set of data points which are collected on regular intervals by fitting low-dimensional models and making forecasts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Message Passing Neural Networks</title>
      <link>http://localhost:1313/posts/message-passing-neural-networks/</link>
      <pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/message-passing-neural-networks/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/boliviainteligente-AXNZb4FEkh8-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@boliviainteligente&#34;&gt;Image by BoliviaInteligente&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Over the past 15 years we have seen a surge of use of Graph Neural Networks (GNNs) being used to model social networks, recommendations systems, transportation networks, and many more systems. With the ever growing use of GNNs, this has naturally led to the questions about the use of GNNs within the medical sector. More specifically, the question of using GNNs to predict the properties of molecules was brought up. Back in the early 2010s, this idea was in its infancy with few successful applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kullback-Leibler Divergence</title>
      <link>http://localhost:1313/posts/kullback-leibler-divergence/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/kullback-leibler-divergence/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/daniela-turcanu-wABdckxoj90-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@protopopica?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Daniela Turcanu&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article will cover the key features of Kullback-Leibler Divergence (KL divergence), a formula invented in 1951 by the mathematicians Soloman Kullback and Richard Leibler. This formula is used in the background of many of the modern day machine learning models focused around probabilistic modelling. These including Variational Autoencoders (VAEs), Generative Models, Reinforcement Learning, and Natural Language Processing. Additionally, this article will cover some of KL divergence&amp;rsquo;s key properties and briefly cover one of its applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Diffusion Models (DDPM)</title>
      <link>http://localhost:1313/posts/diffusion-models-ddpm/</link>
      <pubDate>Fri, 30 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/diffusion-models-ddpm/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/justin-lim-tloFnD-7EpI-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;[Image by Justin Lim]&lt;/p&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;This article will delve into diffusion models, a group of latent variable (see definitions) generative models with applications in image generation, audio synthesis, and denoising. More specifically, this article will mostly focus on the derivations and the ideas behind diffusion models, with a heavy enthuses on the ideas introduced in &lt;a href=&#34;https://arxiv.org/pdf/2006.11239&#34;&gt;Ho et al.&lt;/a&gt; in his Denoising Diffusion Probabilisitic Models paper (DDPMs). The applications of these models will not be covered today.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graph Sage</title>
      <link>http://localhost:1313/posts/graphsage/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/graphsage/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/cajeo-zhang-20JfNRPsMCo-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@cajeo?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Cajeo Zhang&lt;/a&gt;&#xA;Graphs have been used across many fields due to their ability to represent relationships between entities with applications including social networks, search engines, and protein-protein interaction networks. However, one growing limitation of these graphs are the amount of computational resources they require with some large-scale graphs having millions of nodes each with their own set of features and their set of edges.&lt;/p&gt;&#xA;&lt;p&gt;This has led to the creation of graph embedding methods, more specifically the deep embedding methods. These embedding methods aim to create a high-quality representation of the nodes and their edges. Rather than just incorporating the graph structural information into an embedding, these methods also include node and edges features and other hierarchical information. This results in a complicated model which are able to learn very rich representations of nodes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graph Factorisation Methods in Shallow Graphs</title>
      <link>http://localhost:1313/posts/graph-factorisation-methods-in-shallow-graphs/</link>
      <pubDate>Sat, 20 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/graph-factorisation-methods-in-shallow-graphs/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/photo-1706391162070-60a37dee114d.avif&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://unsplash.com/@miracleday?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Elena Mozhvilo&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;&#xA;&lt;p&gt;Graphs are incredibly useful for modelling a range of relationships and interactions. Using nodes to represent entities and edges to represent connections between these entities, they have become a very useful representation tool. Nowadays they are used to model social networks, protein-protein interactions, recommendations systems, knowledge graphs, supply chains, and so much more. However, as these graphs scale up and add more nodes and edges, a range of issues start to arise. They start to become computationally expensive to process, noisy, and difficult to interpret.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Instance Normalisation within GANs</title>
      <link>http://localhost:1313/posts/instance-normalisation-within-gans/</link>
      <pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/instance-normalisation-within-gans/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/justin-simmonds-oGKncrpXn70-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@justsimms?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Justin Simmonds&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Generative Adversarial Networks (GANs) were first introduced in 2014 by Ian Goodfellow in his paper “Generative Adversarial Nets.” This paper presented the GAN framework, which consists of two neural networks called the generator and the discriminator. The generator takes random noise as input and outputs a generated image. The discriminator takes both a generated image and a real image as inputs and tries to determine which is real and which is generated. Their training process can be likened to a ping-pong game, with the generator trying to produce images that fool the discriminator, and the discriminator is trying to identify which images are generated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>U-Net</title>
      <link>http://localhost:1313/posts/u-net/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/u-net/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/photo-1542144950-fa020d965709.avif&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@danielcgold?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Dan Gold&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Machine Learning (ML) has numerous applications in medicine, including disease diagnosis, drug development, predictive healthcare, and more. One key application of ML in medicine is biomedical image processing. These types of models takes an image as an input and then assigns a class label to each pixel in a process called localisation. Competitions are held annually to advance these ML models for biomedical image processing tasks. For instance, the International Symposium on Biomedical Imaging (ISBI) hosts yearly competitions focused on various biomedical imaging challenges. One notable problems from the ISBI involves segmenting neuronal structures in electron microscopy stacks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why Do Trees Outperform Neural Networks on Tabular Data?</title>
      <link>http://localhost:1313/posts/why-do-trees-outperform-neural-networks-on-tabular-data/</link>
      <pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/why-do-trees-outperform-neural-networks-on-tabular-data/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/photo-1458966480358-a0ac42de0a7a.avif&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@toddquackenbush?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Todd Quackenbush&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;For the past 30 years, tree-based algorithms such as Adaboost and Random Forests have been the go-to methods for solving tabular data problems. While neural networks (NNs) have been used in this context, they have historically struggled to match the performance of tree-based methods. Despite recent advancements in NN capabilities and their success in tasks from computer vision, language translation, and image generation, tree-based algorithms still outperform neural networks when it comes to tabular data. This article will introduce several reasons behind the continued dominance of tree-based methods in this domain.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dynamic Programming in Solving Palindrome Partitioning</title>
      <link>http://localhost:1313/posts/dynamic-programming-in-solving-palindrome-partitioning/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/dynamic-programming-in-solving-palindrome-partitioning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/danny-greenberg-cbN8rxHr5S0-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@by_danny_g?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Danny Greenberg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;summary&#34;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;This article explores dynamic programming (DP), a technique used to tackle complex problems in computer science. We will specifically apply DP to two problems involving palindrome partitioning.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;A basic understanding of dynamic programming and some experience with DP problems are recommended before reading this article.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;• &lt;strong&gt;&lt;strong&gt;Palindrome&lt;/strong&gt;&lt;/strong&gt;: A string is a palindrome if it reads the same backward as forward. For example, “aba” is a palindrome, while “aab” is not.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non-Convex Optimisation Learning Rate Scheduling</title>
      <link>http://localhost:1313/posts/non-convex-optimisation-learning-rate-scheduling/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/non-convex-optimisation-learning-rate-scheduling/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@joshuas?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Joshua Sukoff&lt;/a&gt;&#xA;One of the most crucial hyperparameters in any machine learning (ML) model is the learning rate. A small learning rate often results in longer training times and can lead to overfitting. Conversely, a large learning rate may accelerate initial training but risks hindering the model’s convergence to the global minimum and can even cause divergence. Therefore, selecting the appropriate learning rate is a critical step in training any ML model.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
