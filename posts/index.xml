<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on My New Hugo Site</title>
    <link>https://example.org/posts/</link>
    <description>Recent content in Posts on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gambler&#39;s Ruin for a Symmetric Random Walk</title>
      <link>https://example.org/posts/gamblers-ruin-for-a-symmetric-random-walk/</link>
      <pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/gamblers-ruin-for-a-symmetric-random-walk/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/jakob-owens-EroR0Ah9hqI-unsplash.jpg&#34; alt=&#34;Image by Jacob Owens: https://unsplash.com/@jakobowens1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;A gambler starts with i dollars and bets $1 on a fair coin toss, winning $1 on heads and losing $1 on tails. The game stops when the gambler reaches 0 or N dollars.&lt;/p&gt;&#xA;&lt;p&gt;• Derive the probability $p_i$ that the gambler reaches N dollars (i.e. “wins”) before going broke.&lt;/p&gt;&#xA;&lt;p&gt;We&amp;rsquo;ll cover 3 ways to approach this questions.&lt;/p&gt;&#xA;&lt;h2 id=&#34;solution-1&#34;&gt;Solution 1&lt;/h2&gt;&#xA;&lt;p&gt;Let $p_i$ denote the probability of reaching N dollars given he currently has i dollars.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some Questions in Probability II</title>
      <link>https://example.org/posts/some-questions-in-probability-ii/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/some-questions-in-probability-ii/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Some Questions in Probability II&amp;rsquo;&#xA;date = 2025-01-07&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Dice&amp;rdquo;, &amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;, &amp;ldquo;Expectation&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Some Questions in Probability II&amp;rsquo;&#xA;date = 2025-01-07&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Dice&amp;rdquo;, &amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;, &amp;ldquo;Expectation&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Terminating Dice Sequences</title>
      <link>https://example.org/posts/terminating-dice-sequences/</link>
      <pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/terminating-dice-sequences/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Terminating Dice Sequences&amp;rsquo;&#xA;date = 2025-01-02&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Dice&amp;rdquo;, &amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;, &amp;ldquo;Optiver&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Terminating Dice Sequences&amp;rsquo;&#xA;date = 2025-01-02&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Dice&amp;rdquo;, &amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;, &amp;ldquo;Optiver&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>XGBoost</title>
      <link>https://example.org/posts/xgboost/</link>
      <pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/xgboost/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;XGBoost&amp;rsquo;&#xA;date = 2024-12-26&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;tags = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Boosting&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;Decision Trees&amp;rdquo;, &amp;ldquo;Boosting&amp;rdquo;, &amp;ldquo;Gradient Boosting&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;XGBoost&amp;rsquo;&#xA;date = 2024-12-26&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;tags = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Boosting&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;Decision Trees&amp;rdquo;, &amp;ldquo;Boosting&amp;rdquo;, &amp;ldquo;Gradient Boosting&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Perpetual Children</title>
      <link>https://example.org/posts/perpetual-children/</link>
      <pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/perpetual-children/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Perpetual Children&amp;rsquo;&#xA;date = 2024-12-22&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;featuredImage = &amp;ldquo;/images/vivek-kumar-riAKT8Z7ifE-unsplash.jpg&amp;rdquo;&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Perpetual Children&amp;rsquo;&#xA;date = 2024-12-22&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;featuredImage = &amp;ldquo;/images/vivek-kumar-riAKT8Z7ifE-unsplash.jpg&amp;rdquo;&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some Questions in Probability I</title>
      <link>https://example.org/posts/some-questions-in-probability-i/</link>
      <pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/some-questions-in-probability-i/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Some Questions in Probability I&amp;rsquo;&#xA;date = 2024-12-21&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Some Questions in Probability I&amp;rsquo;&#xA;date = 2024-12-21&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>GAMs</title>
      <link>https://example.org/posts/gams/</link>
      <pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/gams/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/anh-vy--kJ542I1oVU-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@anhvygor&#34;&gt;Image by Anh Vy.&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this article we will cover Generalised Additive Models (GAMs). We&amp;rsquo;ll cover the base case by modelling a response variable with a univariate smooth function. We&amp;rsquo;ll then build on this by incorporating multiple exogenous variables to create additive models. After then, the GAM can be covered.&lt;/p&gt;&#xA;&lt;p&gt;For more details about this methods please read the book by Simon N. Wood about Generalised Additive Models.&lt;/p&gt;&#xA;&lt;h2 id=&#34;gams&#34;&gt;GAMs&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;GAMS:&lt;/strong&gt;&lt;/strong&gt; GAMs are a form of generalised linear models with linear response variables that depend on unknown smooth functions of some exogenous variables. These forms of models were initially developed to blend the properties of Generalised Linear Models (GLMs) with additive models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Docker</title>
      <link>https://example.org/posts/docker/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/docker/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/will-turner-iSulLiXzGg0-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@turner_imagery?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Will Turner&lt;/a&gt;&#xA;In this article, we will cover what containerisation means and we look at Docker, a containerisation platform. Furthermore, we will cover the key commands and concepts needed to create your own containers in docker.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-docker&#34;&gt;What is Docker?&lt;/h2&gt;&#xA;&lt;p&gt;Docker is an open-source software platform that assists the deployment of applications. It does this by creating standardised units called containers which are isolated environments containing the application code, runtime, libraries, and any dependencies. These containers are a type of virtual machine that has an OS but does not simulate the entire computer. It is like a sandboxed environment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Martingale Theory</title>
      <link>https://example.org/posts/martingale-theory/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/martingale-theory/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/sq-lim-aWm7fAUNPEc-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@sql&#34;&gt;Image by sq lim&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Today&amp;rsquo;s article will look at one of the more difficult areas in probability called martingale theory. We will cover the basic theory and look at 2 examples which use martingale theory (the drunk man and the ABRACADABRA problem).&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-theory&#34;&gt;The Theory&lt;/h2&gt;&#xA;&lt;p&gt;Before jumping into the problem, we have to cover some of the theory behind martingales. It is important to mention we will be skipping over many of the measure theory definitions and theorems. These will be covered at a later time. If you wish to read up more about this area of mathematics, I&amp;rsquo;d recommend reading the paper by T. Smith &lt;a href=&#34;https://math.uchicago.edu/~may/REU2012/REUPapers/Smith.pdf&#34;&gt;here&lt;/a&gt;. This paper was the basis for this article [1].&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Ant Problem</title>
      <link>https://example.org/posts/the-ant-problem/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/the-ant-problem/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;The Ant Problem&amp;rsquo;&#xA;date = 2024-11-01&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Ants&amp;rdquo;, &amp;ldquo;Ant Problem&amp;rdquo;, &amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;, &amp;ldquo;Optiver&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;The Ant Problem&amp;rsquo;&#xA;date = 2024-11-01&#xA;categories = [&amp;ldquo;Probability&amp;rdquo;]&#xA;tags = [&amp;ldquo;Probability&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Ants&amp;rdquo;, &amp;ldquo;Ant Problem&amp;rdquo;, &amp;ldquo;Probability&amp;rdquo;, &amp;ldquo;Random Walks&amp;rdquo;, &amp;ldquo;Optiver&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rest APIs</title>
      <link>https://example.org/posts/rest-apis/</link>
      <pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/rest-apis/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Rest APIs&amp;rsquo;&#xA;date = 2024-10-30&#xA;categories = [&amp;ldquo;Programming Tools&amp;rdquo;]&#xA;tags = [&amp;ldquo;REST APIs&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Rest&amp;rdquo;, &amp;ldquo;REST APIs&amp;rdquo;, &amp;ldquo;APIs&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Rest APIs&amp;rsquo;&#xA;date = 2024-10-30&#xA;categories = [&amp;ldquo;Programming Tools&amp;rdquo;]&#xA;tags = [&amp;ldquo;REST APIs&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Rest&amp;rdquo;, &amp;ldquo;REST APIs&amp;rdquo;, &amp;ldquo;APIs&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>ARMA &amp; ARIMA</title>
      <link>https://example.org/posts/arma--arima/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/arma--arima/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/elijah-grimm-jSt_ZxlG2bk-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@watchelijah?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Elijah Grimm&lt;/a&gt;&#xA;In this article we will cover the ARMA and ARIMA based statistical models, cover the maths behind these models, and look at the statistical test required prior and the post the model fitting.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;d recommend reading the previous article &lt;a href=&#34;https://mathstoml.ghost.io/what-is-a-time-series-how-can-we-model-it-2/&#34;&gt;here&lt;/a&gt; before reading the remainder of this article. In that article we cover the basics of what is a time series and how we could model it.&lt;/p&gt;&#xA;&lt;h3 id=&#34;definitions&#34;&gt;Definitions:&lt;/h3&gt;&#xA;&lt;p&gt;Recall from our previous article that given a sequence of data points $\{ X_{t}, t \in \mathbb{Z} \}$, the sequence is &lt;strong&gt;&lt;strong&gt;strongly stationary&lt;/strong&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;strong&gt;strictly stationary&lt;/strong&gt;&lt;/strong&gt; if&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is a Time Series &amp; How can we Model it?</title>
      <link>https://example.org/posts/what-is-a-time-series--how-can-we-model-it/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/what-is-a-time-series--how-can-we-model-it/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;What is a Time Series &amp;amp; How can we Model it?&amp;rsquo;&#xA;date = 2024-10-05&#xA;categories = [&amp;ldquo;Time Series&amp;rdquo;]&#xA;tags = [&amp;ldquo;Statistics&amp;rdquo;, &amp;ldquo;Time Series&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Time Series&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;What is a Time Series &amp;amp; How can we Model it?&amp;rsquo;&#xA;date = 2024-10-05&#xA;categories = [&amp;ldquo;Time Series&amp;rdquo;]&#xA;tags = [&amp;ldquo;Statistics&amp;rdquo;, &amp;ldquo;Time Series&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Time Series&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Message Passing Neural Networks</title>
      <link>https://example.org/posts/message-passing-neural-networks/</link>
      <pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/message-passing-neural-networks/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/boliviainteligente-AXNZb4FEkh8-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@boliviainteligente&#34;&gt;Image by BoliviaInteligente&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Over the past 15 years we have seen a surge of use of Graph Neural Networks (GNNs) being used to model social networks, recommendations systems, transportation networks, and many more systems. With the ever growing use of GNNs, this has naturally led to the questions about the use of GNNs within the medical sector. More specifically, the question of using GNNs to predict the properties of molecules was brought up. Back in the early 2010s, this idea was in its infancy with few successful applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kullback-Leibler Divergence</title>
      <link>https://example.org/posts/kullback-leibler-divergence/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/kullback-leibler-divergence/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/daniela-turcanu-wABdckxoj90-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@protopopica?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Daniela Turcanu&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article will cover the key features of Kullback-Leibler Divergence (KL divergence), a formula invented in 1951 by the mathematicians Soloman Kullback and Richard Leibler. This formula is used in the background of many of the modern day machine learning models focused around probabilistic modelling. These including Variational Autoencoders (VAEs), Generative Models, Reinforcement Learning, and Natural Language Processing. Additionally, this article will cover some of KL divergence&amp;rsquo;s key properties and briefly cover one of its applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Diffusion Models (DDPM)</title>
      <link>https://example.org/posts/diffusion-models-ddpm/</link>
      <pubDate>Fri, 30 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/diffusion-models-ddpm/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/justin-lim-tloFnD-7EpI-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;[Image by Justin Lim]&lt;/p&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;This article will delve into diffusion models, a group of latent variable (see definitions) generative models with applications in image generation, audio synthesis, and denoising. More specifically, this article will mostly focus on the derivations and the ideas behind diffusion models, with a heavy enthuses on the ideas introduced in &lt;a href=&#34;https://arxiv.org/pdf/2006.11239&#34;&gt;Ho et al.&lt;/a&gt; in his Denoising Diffusion Probabilisitic Models paper (DDPMs). The applications of these models will not be covered today.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graph Sage</title>
      <link>https://example.org/posts/graphsage/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/graphsage/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/cajeo-zhang-20JfNRPsMCo-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@cajeo?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Cajeo Zhang&lt;/a&gt;&#xA;Graphs have been used across many fields due to their ability to represent relationships between entities with applications including social networks, search engines, and protein-protein interaction networks. However, one growing limitation of these graphs are the amount of computational resources they require with some large-scale graphs having millions of nodes each with their own set of features and their set of edges.&lt;/p&gt;&#xA;&lt;p&gt;This has led to the creation of graph embedding methods, more specifically the deep embedding methods. These embedding methods aim to create a high-quality representation of the nodes and their edges. Rather than just incorporating the graph structural information into an embedding, these methods also include node and edges features and other hierarchical information. This results in a complicated model which are able to learn very rich representations of nodes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graph Factorisation Methods in Shallow Graphs</title>
      <link>https://example.org/posts/graph-factorisation-methods-in-shallow-graphs/</link>
      <pubDate>Sat, 20 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/graph-factorisation-methods-in-shallow-graphs/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/photo-1706391162070-60a37dee114d.avif&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://unsplash.com/@miracleday?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Elena Mozhvilo&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;&#xA;&lt;p&gt;Graphs are incredibly useful for modelling a range of relationships and interactions. Using nodes to represent entities and edges to represent connections between these entities, they have become a very useful representation tool. Nowadays they are used to model social networks, protein-protein interactions, recommendations systems, knowledge graphs, supply chains, and so much more. However, as these graphs scale up and add more nodes and edges, a range of issues start to arise. They start to become computationally expensive to process, noisy, and difficult to interpret.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Instance Normalisation within GANs</title>
      <link>https://example.org/posts/instance-normalisation-within-gans/</link>
      <pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/instance-normalisation-within-gans/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/justin-simmonds-oGKncrpXn70-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@justsimms?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Justin Simmonds&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Generative Adversarial Networks (GANs) were first introduced in 2014 by Ian Goodfellow in his paper “Generative Adversarial Nets.” This paper presented the GAN framework, which consists of two neural networks called the generator and the discriminator. The generator takes random noise as input and outputs a generated image. The discriminator takes both a generated image and a real image as inputs and tries to determine which is real and which is generated. Their training process can be likened to a ping-pong game, with the generator trying to produce images that fool the discriminator, and the discriminator is trying to identify which images are generated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>U-Net</title>
      <link>https://example.org/posts/u-net/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/u-net/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;U-Net&amp;rsquo;&#xA;date = 2024-07-03&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Auto-Encoders&amp;rdquo;]&#xA;tags = [&amp;ldquo;Auto-Encoders&amp;rdquo;]&#xA;keywords = [&amp;ldquo;U-Net&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;AutoEncoders&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;U-Net&amp;rsquo;&#xA;date = 2024-07-03&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Auto-Encoders&amp;rdquo;]&#xA;tags = [&amp;ldquo;Auto-Encoders&amp;rdquo;]&#xA;keywords = [&amp;ldquo;U-Net&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;AutoEncoders&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why Do Trees Outperform Neural Networks on Tabular Data?</title>
      <link>https://example.org/posts/why-do-trees-outperform-neural-networks-on-tabular-data/</link>
      <pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/why-do-trees-outperform-neural-networks-on-tabular-data/</guid>
      <description>&lt;p&gt;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Why Do Trees Outperform Neural Networks on Tabular Data?&amp;rsquo;&#xA;date = 2024-06-27&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;tags = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;Random Forests&amp;rdquo;, &amp;ldquo;Random Forest&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Why Do Trees Outperform Neural Networks on Tabular Data?&amp;rsquo;&#xA;date = 2024-06-27&#xA;categories = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;tags = [&amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;Trees&amp;rdquo;]&#xA;keywords = [&amp;ldquo;Trees&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;, &amp;ldquo;ML&amp;rdquo;, &amp;ldquo;Random Forests&amp;rdquo;, &amp;ldquo;Random Forest&amp;rdquo;]&#xA;description = &amp;ldquo;SEO Description Here&amp;rdquo;&#xA;draft = false&#xA;[params.math]&#xA;math = true&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&#xA;+++&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dynamic Programming in Solving Palindrome Partitioning</title>
      <link>https://example.org/posts/dynamic-programming-in-solving-palindrome-partitioning/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/dynamic-programming-in-solving-palindrome-partitioning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/danny-greenberg-cbN8rxHr5S0-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@by_danny_g?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Danny Greenberg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;summary&#34;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;This article explores dynamic programming (DP), a technique used to tackle complex problems in computer science. We will specifically apply DP to two problems involving palindrome partitioning.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;A basic understanding of dynamic programming and some experience with DP problems are recommended before reading this article.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;• &lt;strong&gt;&lt;strong&gt;Palindrome&lt;/strong&gt;&lt;/strong&gt;: A string is a palindrome if it reads the same backward as forward. For example, “aba” is a palindrome, while “aab” is not.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non-Convex Optimisation Learning Rate Scheduling</title>
      <link>https://example.org/posts/non-convex-optimisation-learning-rate-scheduling/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/non-convex-optimisation-learning-rate-scheduling/</guid>
      <description>&lt;p&gt;+++&#xA;title = &amp;lsquo;Non-Convex Optimisation Learning Rate Scheduling&amp;rsquo;&#xA;draft = false&#xA;date = &amp;lsquo;2024-06-20&amp;rsquo;&#xA;image = &amp;ldquo;/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&amp;rdquo;&#xA;categories = [&amp;ldquo;Programming&amp;rdquo;, &amp;ldquo;AI&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;]&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Non-Convex Optimisation Learning Rate Scheduling&amp;rsquo;&#xA;draft = false&#xA;date = &amp;lsquo;2024-06-20&amp;rsquo;&#xA;image = &amp;ldquo;/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&amp;rdquo;&#xA;categories = [&amp;ldquo;Programming&amp;rdquo;, &amp;ldquo;AI&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;]&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://example.org/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@joshuas?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Joshua Sukoff&lt;/a&gt;&#xA;One of the most crucial hyperparameters in any machine learning (ML) model is the learning rate. A small learning rate often results in longer training times and can lead to overfitting. Conversely, a large learning rate may accelerate initial training but risks hindering the model’s convergence to the global minimum and can even cause divergence. Therefore, selecting the appropriate learning rate is a critical step in training any ML model.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
