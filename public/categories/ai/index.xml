<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on My New Hugo Site</title>
    <link>https://example.org/categories/ai/</link>
    <description>Recent content in AI on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Diffusion Models (DDPM)</title>
      <link>https://example.org/posts/diffusion-models-ddpm/</link>
      <pubDate>Fri, 30 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/diffusion-models-ddpm/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://example.org/images/justin-lim-tloFnD-7EpI-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;[Image by Justin Lim]&lt;/p&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;This article will delve into diffusion models, a group of latent variable (see definitions) generative models with applications in image generation, audio synthesis, and denoising. More specifically, this article will mostly focus on the derivations and the ideas behind diffusion models, with a heavy enthuses on the ideas introduced in &lt;a href=&#34;https://arxiv.org/pdf/2006.11239&#34;&gt;Ho et al.&lt;/a&gt; in his Denoising Diffusion Probabilisitic Models paper (DDPMs). The applications of these models will not be covered today.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non-Convex Optimisation Learning Rate Scheduling</title>
      <link>https://example.org/posts/non-convex-optimisation-learning-rate-scheduling/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/non-convex-optimisation-learning-rate-scheduling/</guid>
      <description>&lt;p&gt;+++&#xA;title = &amp;lsquo;Non-Convex Optimisation Learning Rate Scheduling&amp;rsquo;&#xA;draft = false&#xA;date = &amp;lsquo;2024-06-20&amp;rsquo;&#xA;image = &amp;ldquo;/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&amp;rdquo;&#xA;categories = [&amp;ldquo;Programming&amp;rdquo;, &amp;ldquo;AI&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;]&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;+++&#xA;title = &amp;lsquo;Non-Convex Optimisation Learning Rate Scheduling&amp;rsquo;&#xA;draft = false&#xA;date = &amp;lsquo;2024-06-20&amp;rsquo;&#xA;image = &amp;ldquo;/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&amp;rdquo;&#xA;categories = [&amp;ldquo;Programming&amp;rdquo;, &amp;ldquo;AI&amp;rdquo;, &amp;ldquo;Machine Learning&amp;rdquo;]&#xA;+++&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://example.org/images/joshua-sukoff-xTSwjDonDhE-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@joshuas?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Joshua Sukoff&lt;/a&gt;&#xA;One of the most crucial hyperparameters in any machine learning (ML) model is the learning rate. A small learning rate often results in longer training times and can lead to overfitting. Conversely, a large learning rate may accelerate initial training but risks hindering the modelâ€™s convergence to the global minimum and can even cause divergence. Therefore, selecting the appropriate learning rate is a critical step in training any ML model.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
