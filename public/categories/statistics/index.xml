<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on My New Hugo Site</title>
    <link>http://localhost:1313/categories/statistics/</link>
    <description>Recent content in Statistics on My New Hugo Site</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What is a Time Series &amp; How can we Model it?</title>
      <link>http://localhost:1313/posts/what-is-a-time-series--how-can-we-model-it/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/what-is-a-time-series--how-can-we-model-it/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/oguzhan-kiran-wj0l2BJKmkU-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://mathstoml.ghost.io/ghost/#/editor/post/66eb4a6473445600016630bb&#34;&gt;Image by Oğuzhan Kıran&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article will cover what is a time series, what does it mean for a time series to be stationary, and how could an autoregressive process or moving average process be used to model the time series.&lt;/p&gt;&#xA;&lt;p&gt;Before starting this article, I would like to mention the Cambridge notes from &lt;a href=&#34;https://www.statslab.cam.ac.uk/~rrw1/timeseries/t.pdf&#34;&gt;here&lt;/a&gt; form the foundation of this content.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-a-time-series&#34;&gt;What is a Time Series?&lt;/h2&gt;&#xA;&lt;p&gt;In short, time series is area of statistics which focuses on describing a set of data points which are collected on regular intervals by fitting low-dimensional models and making forecasts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kullback-Leibler Divergence</title>
      <link>http://localhost:1313/posts/kullback-leibler-divergence/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/kullback-leibler-divergence/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/daniela-turcanu-wABdckxoj90-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@protopopica?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Daniela Turcanu&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article will cover the key features of Kullback-Leibler Divergence (KL divergence), a formula invented in 1951 by the mathematicians Soloman Kullback and Richard Leibler. This formula is used in the background of many of the modern day machine learning models focused around probabilistic modelling. These including Variational Autoencoders (VAEs), Generative Models, Reinforcement Learning, and Natural Language Processing. Additionally, this article will cover some of KL divergence&amp;rsquo;s key properties and briefly cover one of its applications.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
